{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"Pre-Processed_Dataset.csv\").reset_index(drop=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "data = data.drop(columns=[\"RED_ELO_BEFORE\", \"BLUE_ELO_BEFORE\", \"RED_ELO_PEAK\", \"BLUE_ELO_PEAK\"])\n",
    "\n",
    "# Convert fightTime columns to numeric\n",
    "data[\"red_fightTime\"] = pd.to_numeric(data[\"red_fightTime\"], errors=\"coerce\")\n",
    "data[\"blue_fightTime\"] = pd.to_numeric(data[\"blue_fightTime\"], errors=\"coerce\")\n",
    "\n",
    "# Filter outcome\n",
    "data = data.loc[data['OUTCOME'].isin(['Red', 'Blue'])]\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder_outcome = LabelEncoder()\n",
    "label_encoder_method = LabelEncoder()\n",
    "data['OUTCOME'] = label_encoder_outcome.fit_transform(data['OUTCOME'])\n",
    "data['METHOD'] = label_encoder_method.fit_transform(data['METHOD'])\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=[\"OUTCOME\", \"METHOD\", \"BOUT\", \"EVENT\", \"WEIGHTCLASS\", \"REFEREE\", \"DETAILS\", \"URL\", \"TIME FORMAT\", \"TIME\", \"Unnamed: 0\", \"ROUND\", \"Unnamed: 0.1\"])\n",
    "y = data[\"OUTCOME\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train initial Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=7, min_samples_split=20, min_samples_leaf=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "# Filter out low-importance features\n",
    "threshold = 0.005  # tune this threshold\n",
    "low_importance_features = importance_df[importance_df[\"Importance\"] < threshold][\"Feature\"].tolist()\n",
    "print(f\"\\nDropping {len(low_importance_features)} low-importance features (below {threshold}):\")\n",
    "print(low_importance_features)\n",
    "\n",
    "# Drop low-importance features\n",
    "X = X.drop(columns=low_importance_features)\n",
    "\n",
    "# Redo split on cleaned data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Retrain Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=7, min_samples_split=20, min_samples_leaf=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf_test = rf.predict(X_test)\n",
    "\n",
    "# Train XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=5.0,\n",
    "    reg_alpha=2.0,\n",
    "    gamma=4.0,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_test = xgb.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\nRandom Forest Accuracy: {accuracy_score(y_test, y_pred_rf_test):.4f}\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb_test):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report - Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf_test))\n",
    "\n",
    "print(\"\\nClassification Report - XGBoost:\")\n",
    "print(classification_report(y_test, y_pred_xgb_test))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y.values.ravel(), cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# Feature Importance Plot\n",
    "final_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(final_importance_df[\"Feature\"], final_importance_df[\"Importance\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
